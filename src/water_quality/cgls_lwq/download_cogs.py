"""
Download the Copernicus Global Land Service Lake Water Quality datasets,
crop and convert to Cloud Optimized Geotiffs, and push to an S3 bucket.
"""

import logging
import sys
import warnings
from datetime import datetime

import click
import numpy as np
import requests
import rioxarray
from datacube.testutils.io import rio_slurp_xarray
from odc.geo.xr import assign_crs
from rasterio.errors import NotGeoreferencedWarning
from tqdm import tqdm

from water_quality.cgls_lwq.constants import AFRICA_BBOX, MANIFEST_FILE_URLS, MEASUREMENTS
from water_quality.cgls_lwq.netcdf import (
    get_netcdf_subdatasets_uris,
    parse_netcdf_subdatasets_uri,
    parse_netcdf_url,
)
from water_quality.cgls_lwq.tiles import get_africa_tiles, get_tile_index_str_from_tuple
from water_quality.io import (
    check_directory_exists,
    check_file_exists,
    get_filesystem,
    is_local_path,
    join_urlpath,
)
from water_quality.logs import logging_setup

# Suppress the warning
warnings.filterwarnings("ignore", category=NotGeoreferencedWarning)


def get_output_cog_url(
    output_dir: str, netcdf_subdataset_uri: str, tile_index: tuple[int, int] | None = None
) -> str:
    """
    Create output file path for COG generated by cropping CGLS LWQ netcdf subdataset
    to tile bounds.

    Parameters
    ----------
    output_dir : str
        Directory to write the CoG file to.
    netcdf_subdataset_uri : str
        CGLS LWQ netcdf subdataset uri to generate CoG file from.
    tile_index : tuple[int, int] | None, optional
        Tile index of tile to crop the CGLS LWQ netcdf subdataset to, by default None

    Returns
    -------
    str
        CoG output file path.
    """
    _, _, netcdf_url, subdataset_variable = parse_netcdf_subdatasets_uri(netcdf_subdataset_uri)
    filename_prefix, acronym, date_str, area, sensor, version, _ = parse_netcdf_url(netcdf_url)
    date = datetime.strptime(date_str, "%Y%m%d%H%M%S")
    year = str(date.year)
    month = f"{date.month:02d}"

    if tile_index:
        tile_index_str = get_tile_index_str_from_tuple(tile_index)
        tile_index_str_x, tile_index_str_y = tile_index_str.split("_")
        parent_dir = join_urlpath(output_dir, tile_index_str_x, tile_index_str_y, year, month)
        file_name = f"{filename_prefix}_{acronym}_{date_str}_{area}_{sensor}_{version}_{subdataset_variable}_{tile_index_str_x}{tile_index_str_y}.tif"
    else:
        parent_dir = join_urlpath(output_dir, year, month)
        file_name = f"{filename_prefix}_{acronym}_{date_str}_{area}_{sensor}_{version}_{subdataset_variable}.tif"

    if not check_directory_exists(parent_dir):
        fs = get_filesystem(parent_dir, anon=False)
        fs.makedirs(parent_dir, exist_ok=True)

    output_cog_url = join_urlpath(parent_dir, file_name)
    return output_cog_url


@click.command(
    "download-cogs",
    help="Download the Copernicus Global Land Service Lake Water Quality datasets,"
    "crop and convert to Cloud Optimized Geotiffs, and push to an S3 bucket.",
    no_args_is_help=True,
)
@click.option(
    "--product-name",
    type=click.Choice(list(MANIFEST_FILE_URLS.keys()), case_sensitive=True),
    help="Name of the product to generate the stac item files for",
)
@click.option(
    "--cog-output-dir",
    type=str,
    help="Directory to write the cog files to",
)
@click.option("--overwrite/--no-overwrite", default=False, show_default=True)
@click.option(
    "--max-parallel-steps",
    default=1,
    show_default=True,
    type=int,
    help="Maximum number of parallel steps/pods to have in the workflow.",
)
@click.option(
    "--worker-idx",
    default=0,
    show_default=True,
    type=int,
    help="Sequential index which will be used to define the range of geotiffs the pod will work with.",
)
@click.option("-v", "--verbose", default=1, count=True)
def download_cogs(
    product_name: str,
    cog_output_dir: str,
    overwrite: bool,
    max_parallel_steps: int,
    worker_idx: int,
    verbose: int,
):
    # Setup logging level
    logging_setup(verbose)
    log = logging.getLogger(__name__)

    if product_name not in MANIFEST_FILE_URLS.keys():
        raise NotImplementedError(
            f"Manifest file url not configured for the product {product_name}"
        )

    # Read urls available for the product
    r = requests.get(MANIFEST_FILE_URLS[product_name])
    netcdf_urls = r.text.splitlines()

    # TODO: Remove filter by year
    # filter to 2011 only
    netcdf_urls = [i for i in netcdf_urls if "/20111121/" in i]
    log.info(f"Found {len(netcdf_urls)} netcdf urls in the manifest file")

    # Split files equally among the workers
    task_chunks = np.array_split(np.array(netcdf_urls), max_parallel_steps)
    task_chunks = [chunk.tolist() for chunk in task_chunks]
    task_chunks = list(filter(None, task_chunks))

    # In case of the index being bigger than the number of positions in the array, the extra POD isn't necessary
    if len(task_chunks) <= worker_idx:
        log.warning(f"Worker {worker_idx} Skipped!")
        sys.exit(0)

    log.info(f"Executing worker {worker_idx}")

    dataset_paths = task_chunks[worker_idx]

    log.info(f"Generating COG files for the product {product_name}")

    # Define the tiles over Africa
    # Select resolution for tile
    if "300m" in netcdf_urls[0]:
        grid_res = 300
    elif "100m" in netcdf_urls[0]:
        grid_res = 100

    tiles = get_africa_tiles(grid_res)

    for idx, netdf_url in enumerate(dataset_paths):
        log.info(f"Generating cog files for {netdf_url} {idx + 1}/{len(dataset_paths)}")

        # Get the subdatasets in the netcdf
        netcdf_subdatasets_uris = get_netcdf_subdatasets_uris(netdf_url)
        # Filter by required measurements
        netcdf_subdatasets_uris = {
            k: v for k, v in netcdf_subdatasets_uris.items() if k in MEASUREMENTS
        }
        # Check
        assert len(netcdf_subdatasets_uris) == len(MEASUREMENTS)

        for var, subdataset_uri in netcdf_subdatasets_uris.items():
            # Get attributes to be used in tiled COGs
            common_attrs = rioxarray.open_rasterio(subdataset_uri).attrs
            exclude = ["lon#", "lat#", "number_of_regions", "TileSize", "NETCDF"]
            filtered_attrs = {
                k: v
                for k, v in common_attrs.items()
                if not any(sub.lower() in k.lower() for sub in exclude)
            }

            # Crop the netcdf subdataset to each tile
            with tqdm(iterable=tiles, desc=f"Cropping {var} subdataset", total=len(tiles)) as tiles:
                for tile in tiles:
                    tile_idx, tile_geobox = tile
                    output_cog_url = get_output_cog_url(cog_output_dir, subdataset_uri, tile_idx)
                    if not overwrite:
                        if check_file_exists(output_cog_url):
                            continue

                    da = rio_slurp_xarray(subdataset_uri, tile_geobox)

                    # Write cog files
                    if is_local_path(output_cog_url):
                        da.odc.write_cog(
                            fname=output_cog_url, overwrite=overwrite, tags=filtered_attrs
                        )
                    else:
                        cog_bytes = da.odc.write_cog(
                            fname=":mem:", overwrite=overwrite, tags=filtered_attrs
                        )
                        fs = get_filesystem(output_cog_url, anon=False)
                        with fs.open(output_cog_url, "wb") as f:
                            f.write(cog_bytes)

            log.info(f"Written COGs for {subdataset_uri}")
